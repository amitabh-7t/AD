{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Component 3: Preprocessing & Data Pipelines",
        "",
        "Define preprocessing variants, augmentation, and build tf.data pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf",
        "import pandas as pd",
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "import os",
        "",
        "SEED = 42",
        "tf.random.set_seed(SEED)",
        "np.random.seed(SEED)",
        "",
        "IMG_SIZE = (224, 224)",
        "BATCH_SIZE = 32",
        "",
        "# Load manifests",
        "train_df = pd.read_csv('../outputs/train_manifest.csv')",
        "val_df = pd.read_csv('../outputs/val_manifest.csv')",
        "test_df = pd.read_csv('../outputs/test_manifest.csv')",
        "",
        "print(f'Train: {len(train_df)} images')",
        "print(f'Val:   {len(val_df)} images')",
        "print(f'Test:  {len(test_df)} images')",
        "print(f'Classes: {len(train_df[\"class_label\"].unique())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_resize_rescale(filepath, label):",
        "    \"\"\"Simple resize and rescale to [0,1].\"\"\"",
        "    img = tf.io.read_file(filepath)",
        "    img = tf.image.decode_jpeg(img, channels=3)",
        "    img = tf.image.resize(img, IMG_SIZE)",
        "    img = img / 255.0",
        "    return img, label",
        "",
        "def preprocess_resnet(filepath, label):",
        "    \"\"\"ResNet50-specific preprocessing.\"\"\"",
        "    img = tf.io.read_file(filepath)",
        "    img = tf.image.decode_jpeg(img, channels=3)",
        "    img = tf.image.resize(img, IMG_SIZE)",
        "    img = tf.keras.applications.resnet50.preprocess_input(img)",
        "    return img, label",
        "",
        "def preprocess_efficientnet(filepath, label):",
        "    \"\"\"EfficientNet-specific preprocessing.\"\"\"",
        "    img = tf.io.read_file(filepath)",
        "    img = tf.image.decode_jpeg(img, channels=3)",
        "    img = tf.image.resize(img, IMG_SIZE)",
        "    img = tf.keras.applications.efficientnet.preprocess_input(img)",
        "    return img, label",
        "",
        "def preprocess_densenet(filepath, label):",
        "    \"\"\"DenseNet-specific preprocessing.\"\"\"",
        "    img = tf.io.read_file(filepath)",
        "    img = tf.image.decode_jpeg(img, channels=3)",
        "    img = tf.image.resize(img, IMG_SIZE)",
        "    img = tf.keras.applications.densenet.preprocess_input(img)",
        "    return img, label",
        "",
        "print(\"\u2713 Preprocessing functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define augmentation pipeline",
        "augmentation = tf.keras.Sequential([",
        "    tf.keras.layers.RandomFlip('horizontal'),",
        "    tf.keras.layers.RandomRotation(0.2),",
        "    tf.keras.layers.RandomZoom(0.2),",
        "    tf.keras.layers.RandomContrast(0.2)",
        "], name='augmentation')",
        "",
        "print(\"\u2713 Augmentation pipeline created\")",
        "print(\"  - RandomFlip (horizontal)\")",
        "print(\"  - RandomRotation (\u00b120%)\")",
        "print(\"  - RandomZoom (\u00b120%)\")",
        "print(\"  - RandomContrast (\u00b120%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Build tf.data Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_dataset(dataframe, preprocess_fn, augment=False, shuffle=True, batch_size=BATCH_SIZE):",
        "    \"\"\"Build optimized tf.data pipeline.\"\"\"",
        "    filepaths = dataframe['filepath'].values",
        "    labels = dataframe['class_label'].values",
        "    ",
        "    # Create dataset",
        "    ds = tf.data.Dataset.from_tensor_slices((filepaths, labels))",
        "    ",
        "    # Preprocessing",
        "    ds = ds.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)",
        "    ",
        "    # Cache before augmentation",
        "    ds = ds.cache()",
        "    ",
        "    # Augmentation (only for training)",
        "    if augment:",
        "        ds = ds.map(lambda x, y: (augmentation(x, training=True), y), ",
        "                   num_parallel_calls=tf.data.AUTOTUNE)",
        "    ",
        "    # Shuffle",
        "    if shuffle:",
        "        ds = ds.shuffle(buffer_size=1000, seed=SEED)",
        "    ",
        "    # Batch and prefetch",
        "    ds = ds.batch(batch_size)",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)",
        "    ",
        "    return ds",
        "",
        "print(\"\u2713 Dataset building function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Create Datasets (Basic Preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build datasets with simple resize+rescale",
        "train_ds = build_dataset(train_df, preprocess_resize_rescale, augment=True, shuffle=True)",
        "val_ds = build_dataset(val_df, preprocess_resize_rescale, augment=False, shuffle=False)",
        "test_ds = build_dataset(test_df, preprocess_resize_rescale, augment=False, shuffle=False)",
        "",
        "print(\"\u2713 Datasets created\")",
        "print(f\"  Train: {len(train_df)} images, {len(train_ds)} batches\")",
        "print(f\"  Val:   {len(val_df)} images, {len(val_ds)} batches\")",
        "print(f\"  Test:  {len(test_df)} images, {len(test_ds)} batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 Test Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the pipeline",
        "print(\"Testing pipeline with a sample batch...\")",
        "for images, labels in train_ds.take(1):",
        "    print(f\"\\nBatch shape: {images.shape}\")",
        "    print(f\"Labels shape: {labels.shape}\")",
        "    print(f\"Image dtype: {images.dtype}\")",
        "    print(f\"Image range: [{images.numpy().min():.3f}, {images.numpy().max():.3f}]\")",
        "    print(f\"Labels: {labels.numpy()}\")",
        "    ",
        "    # Visualize a few samples",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))",
        "    for i, ax in enumerate(axes.flatten()):",
        "        if i < images.shape[0]:",
        "            ax.imshow(images[i].numpy())",
        "            ax.set_title(f'Label: {labels[i].numpy()}')",
        "            ax.axis('off')",
        "    plt.suptitle('Sample Batch from Training Pipeline', fontsize=14, fontweight='bold')",
        "    plt.tight_layout()",
        "    plt.show()",
        "",
        "print(\"\\n\" + \"=\"*60)",
        "print(\"\u2705 PREPROCESSING COMPLETE\")",
        "print(\"=\"*60)",
        "print(\"\\nDatasets are ready for model training!\")",
        "print(\"\\nAvailable preprocessing functions:\")",
        "print(\"  - preprocess_resize_rescale (for Baseline CNN)\")",
        "print(\"  - preprocess_resnet (for ResNet50)\")",
        "print(\"  - preprocess_efficientnet (for EfficientNetB0)\")",
        "print(\"  - preprocess_densenet (for DenseNet121)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}