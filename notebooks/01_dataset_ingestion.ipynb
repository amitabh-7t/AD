{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component 1: Dataset Ingestion and Summary\n",
    "\n",
    "This notebook:\n",
    "- Loads all 11,743 MRI images\n",
    "- Validates file integrity using SHA256 hashing\n",
    "- Creates stratified train/val/test splits (70/15/15)\n",
    "- Generates dataset manifest CSV\n",
    "- Visualizes class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = '../data'\n",
    "OUTPUT_DIR = '../outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Imports successful\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Random seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Images and Compute Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_sha256(filepath):\n",
    "    \"\"\"Compute SHA256 hash of file.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b''):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def load_dataset_manifest(data_dir):\n",
    "    \"\"\"Load all images and create manifest.\"\"\"\n",
    "    \n",
    "    # Get class directories\n",
    "    class_dirs = sorted([d for d in os.listdir(data_dir) \n",
    "                        if os.path.isdir(os.path.join(data_dir, d)) and not d.startswith('.')])\n",
    "    \n",
    "    print(f\"Found {len(class_dirs)} classes: {class_dirs}\")\n",
    "    \n",
    "    # Map class names\n",
    "    class_to_label = {name: idx for idx, name in enumerate(class_dirs)}\n",
    "    \n",
    "    data = []\n",
    "    corrupted = []\n",
    "    \n",
    "    for class_name in class_dirs:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        image_files = [f for f in os.listdir(class_dir) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        print(f\"\\nProcessing {class_name}: {len(image_files)} images\")\n",
    "        \n",
    "        for img_file in tqdm(image_files, desc=class_name):\n",
    "            filepath = os.path.join(class_dir, img_file)\n",
    "            \n",
    "            try:\n",
    "                # Try to open image\n",
    "                img = Image.open(filepath)\n",
    "                img.verify()  # Verify it's not corrupted\n",
    "                img = Image.open(filepath)  # Reopen after verify\n",
    "                \n",
    "                # Get metadata\n",
    "                width, height = img.size\n",
    "                file_size = os.path.getsize(filepath)\n",
    "                sha256 = compute_sha256(filepath)\n",
    "                \n",
    "                data.append({\n",
    "                    'filepath': filepath,\n",
    "                    'class_name': class_name,\n",
    "                    'class_label': class_to_label[class_name],\n",
    "                    'width': width,\n",
    "                    'height': height,\n",
    "                    'file_size_bytes': file_size,\n",
    "                    'sha256_hash': sha256\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n⚠️  Corrupted file: {filepath} - {e}\")\n",
    "                corrupted.append(filepath)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset Summary:\")\n",
    "    print(f\"  Total images: {len(df)}\")\n",
    "    print(f\"  Corrupted files: {len(corrupted)}\")\n",
    "    print(f\"  Classes: {len(class_dirs)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return df, corrupted\n",
    "\n",
    "# Load dataset\n",
    "df, corrupted_files = load_dataset_manifest(DATA_DIR)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Save Dataset Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save manifest\n",
    "manifest_path = os.path.join(OUTPUT_DIR, 'dataset_manifest.csv')\n",
    "df.to_csv(manifest_path, index=False)\n",
    "print(f\"✓ Dataset manifest saved to: {manifest_path}\")\n",
    "\n",
    "# Save corrupted files list if any\n",
    "if corrupted_files:\n",
    "    corrupted_path = os.path.join(OUTPUT_DIR, 'corrupted_files.txt')\n",
    "    with open(corrupted_path, 'w') as f:\n",
    "        f.write('\\n'.join(corrupted_files))\n",
    "    print(f\"✓ Corrupted files list saved to: {corrupted_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Class counts\n",
    "class_counts = df['class_name'].value_counts().sort_index()\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(class_counts)\n",
    "print(f\"\\nPercentages:\")\n",
    "print((class_counts / len(df) * 100).round(2))\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "class_counts.plot(kind='bar', ax=ax1, color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "ax1.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Number of Images', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Class Distribution - Counts', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    ax1.text(i, v + 100, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors = sns.color_palette('husl', len(class_counts))\n",
    "ax2.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%',\n",
    "       colors=colors, startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax2.set_title('Class Distribution - Percentages', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(OUTPUT_DIR, 'class_distribution.png')\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Class distribution plot saved to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Create Stratified Splits (70/15/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# First split: 70% train, 30% temp\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.30, stratify=df['class_label'], random_state=SEED\n",
    ")\n",
    "\n",
    "# Second split: 50% of temp for val, 50% for test (15% each of total)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.50, stratify=temp_df['class_label'], random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Splits:\")\n",
    "print(f\"  Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification\n",
    "print(f\"\\nClass distribution in splits:\")\n",
    "split_dist = pd.DataFrame({\n",
    "    'Train': train_df['class_name'].value_counts().sort_index(),\n",
    "    'Val': val_df['class_name'].value_counts().sort_index(),\n",
    "    'Test': test_df['class_name'].value_counts().sort_index()\n",
    "})\n",
    "print(split_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Save Split Manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save split manifests\n",
    "train_path = os.path.join(OUTPUT_DIR, 'train_manifest.csv')\n",
    "val_path = os.path.join(OUTPUT_DIR, 'val_manifest.csv')\n",
    "test_path = os.path.join(OUTPUT_DIR, 'test_manifest.csv')\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"✓ Train manifest saved to: {train_path}\")\n",
    "print(f\"✓ Val manifest saved to: {val_path}\")\n",
    "print(f\"✓ Test manifest saved to: {test_path}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✅ COMPONENT 1 COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nArtifacts created:\")\n",
    "print(f\"  - {manifest_path}\")\n",
    "print(f\"  - {train_path}\")\n",
    "print(f\"  - {val_path}\")\n",
    "print(f\"  - {test_path}\")\n",
    "print(f\"  - {plot_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
