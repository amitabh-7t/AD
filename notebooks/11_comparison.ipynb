{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Component 11: Model Comparison",
        "",
        "Compare all models by metrics, size, and inference speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd",
        "import matplotlib.pyplot as plt",
        "import os",
        "",
        "results = pd.read_csv('../outputs/evaluation/all_models_metrics.csv')",
        "print('Model Comparison:')",
        "print(results)",
        "",
        "# Model sizes",
        "for model_name in results['model']:",
        "    path = f'../outputs/models/{model_name}_best.h5'",
        "    if os.path.exists(path):",
        "        size_mb = os.path.getsize(path) / (1024**2)",
        "        print(f'{model_name}: {size_mb:.2f} MB')",
        "",
        "# Plot comparison",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))",
        "metrics = ['accuracy', 'balanced_accuracy', 'macro_f1', 'weighted_f1']",
        "for i, metric in enumerate(metrics):",
        "    ax = axes.flatten()[i]",
        "    results.plot(x='model', y=metric, kind='bar', ax=ax, legend=False)",
        "    ax.set_title(metric.replace('_', ' ').title())",
        "    ax.set_xlabel('')",
        "    ax.grid(alpha=0.3)",
        "plt.tight_layout()",
        "plt.savefig('../outputs/comparison/model_comparison.png', dpi=200)",
        "plt.show()",
        "print('\u2705 Comparison complete')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}